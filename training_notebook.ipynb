{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GRU, Bidirectional, TimeDistributed, Conv1D, MaxPooling1D, BatchNormalization, Attention, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import mixed_precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set mixed precision policy\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Load saved features\n",
    "train_df = pd.read_pickle('d:/data/train_features.pkl')\n",
    "adapt_df = pd.read_pickle('d:/data/adapt_features.pkl')\n",
    "\n",
    "# Remove any rows with NaN values in 'transcript'\n",
    "train_df.dropna(subset=['transcript'], inplace=True)\n",
    "adapt_df.dropna(subset=['transcript'], inplace=True)\n",
    "\n",
    "# Tokenize the transcriptions\n",
    "all_transcriptions = train_df['transcript'].tolist() + adapt_df['transcript'].tolist()\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(all_transcriptions)\n",
    "\n",
    "train_df['text_seq'] = tokenizer.texts_to_sequences(train_df['transcript'])\n",
    "adapt_df['text_seq'] = tokenizer.texts_to_sequences(adapt_df['transcript'])\n",
    "\n",
    "# Determine the maximum length of the features\n",
    "max_feature_length = max(train_df['features'].apply(len).max(), adapt_df['features'].apply(len).max())\n",
    "\n",
    "# Pad the features and text sequences\n",
    "X_train = pad_sequences(train_df['features'].tolist(), maxlen=max_feature_length, padding='post', dtype='float32')\n",
    "X_adapt = pad_sequences(adapt_df['features'].tolist(), maxlen=max_feature_length, padding='post', dtype='float32')\n",
    "\n",
    "# Define a fixed sequence length for padding the text sequences\n",
    "fixed_sequence_length = 589  # This should match the model output sequence length\n",
    "\n",
    "# Pad the text sequences to the fixed sequence length\n",
    "y_train = pad_sequences(train_df['text_seq'].tolist(), maxlen=fixed_sequence_length, padding='post')\n",
    "y_adapt = pad_sequences(adapt_df['text_seq'].tolist(), maxlen=fixed_sequence_length, padding='post')\n",
    "\n",
    "# Convert text sequences to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_adapt = np.array(y_adapt)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 for the padding token\n",
    "\n",
    "# Build the enhanced model\n",
    "def build_model(input_dim, output_dim, rnn_units=256, use_attention=True):\n",
    "    input_data = Input(name='input', shape=(None, input_dim))\n",
    "    \n",
    "    # Convolutional layer\n",
    "    x = Conv1D(filters=256, kernel_size=13, strides=1, padding='same', activation='relu')(input_data)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Bidirectional GRU layers with layer normalization and dropout\n",
    "    x = Bidirectional(GRU(rnn_units, return_sequences=True))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(rnn_units, return_sequences=True))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Apply attention if enabled\n",
    "    if use_attention:\n",
    "        x = Attention()([x, x])\n",
    "    \n",
    "    # TimeDistributed Dense layer for output\n",
    "    y_pred = TimeDistributed(Dense(output_dim, activation='softmax'))(x)\n",
    "    \n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    return model\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "output_dim = vocab_size  # Updated to match the vocabulary size\n",
    "\n",
    "# Compile the model\n",
    "model = build_model(input_dim, output_dim)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Split the data\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create tf.data.Datasets\n",
    "batch_size = 16\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_split, y_train_split)).shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_split, y_val_split)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Define the WER calculation function\n",
    "def wer(reference, hypothesis):\n",
    "    r = reference.split()\n",
    "    h = hypothesis.split()\n",
    "    d = np.zeros((len(r) + 1, len(h) + 1), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(len(r) + 1):\n",
    "        for j in range(len(h) + 1):\n",
    "            if i == 0:\n",
    "                d[i][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][j] = i\n",
    "            else:\n",
    "                d[i][j] = min(d[i - 1][j] + 1,\n",
    "                              d[i][j - 1] + 1,\n",
    "                              d[i - 1][j - 1] + (r[i - 1] != h[j - 1]))\n",
    "    \n",
    "    return d[len(r)][len(h)]\n",
    "\n",
    "# Decode predictions function\n",
    "def decode_predictions(predictions, tokenizer, max_texts=10):\n",
    "    decoded_texts = []\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    index_word[0] = ''  # padding index\n",
    "    \n",
    "    for pred in predictions[:max_texts]:  # Limit to max_texts for printing\n",
    "        decoded_indices = np.argmax(pred, axis=-1)\n",
    "        decoded_text = ''.join([index_word.get(i, '') for i in decoded_indices])\n",
    "        decoded_texts.append(decoded_text.strip())\n",
    "    \n",
    "    return decoded_texts\n",
    "\n",
    "# Create the custom callback for WER\n",
    "class WERCallback(Callback):\n",
    "    def __init__(self, val_data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.val_data = val_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, val_y = self.val_data\n",
    "        predictions = self.model.predict(val_x)\n",
    "        decoded_predictions = decode_predictions(predictions, self.tokenizer)\n",
    "        references = [self.tokenizer.sequences_to_texts([seq])[0] for seq in val_y]\n",
    "\n",
    "        total_wer = 0\n",
    "        num_samples = len(references)\n",
    "        \n",
    "        for ref, hyp in zip(references, decoded_predictions):\n",
    "            total_wer += wer(ref, hyp)\n",
    "        \n",
    "        avg_wer = total_wer / num_samples\n",
    "        print(f\"\\nEpoch {epoch + 1}: Validation WER: {avg_wer:.4f}\")\n",
    "        \n",
    "        # Print the first 10 decoded texts\n",
    "        print(\"First 10 decoded texts:\")\n",
    "        for text in decoded_predictions[:10]:\n",
    "            print(text)\n",
    "\n",
    "# Callbacks\n",
    "wer_callback = WERCallback((X_val_split, y_val_split), tokenizer)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_model.weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=9,  # Increase epochs to allow for more training\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping, checkpoint, wer_callback]\n",
    ")\n",
    "\n",
    "# Fine-tune on adaptation data\n",
    "X_adapt_split, X_adapt_val_split, y_adapt_split, y_adapt_val_split = train_test_split(X_adapt, y_adapt, test_size=0.2, random_state=42)\n",
    "adapt_dataset = tf.data.Dataset.from_tensor_slices((X_adapt_split, y_adapt_split)).shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "adapt_val_dataset = tf.data.Dataset.from_tensor_slices((X_adapt_val_split, y_adapt_val_split)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Fine-tune the model on the adaptation data\n",
    "fine_tune_history = model.fit(\n",
    "    adapt_dataset,\n",
    "    epochs=9,  # You can adjust the number of epochs for fine-tuning\n",
    "    validation_data=adapt_val_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping, checkpoint, wer_callback]\n",
    ")\n",
    "\n",
    "# Save model checkpoints\n",
    "model.save('d:/data/model_checkpoint')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
